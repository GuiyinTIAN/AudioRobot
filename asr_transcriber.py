import os
import time
from datetime import datetime
import torch
from faster_whisper import WhisperModel


class ASRTranscriber:
    """
    ASR transcription service using Whisper models.
    
    Handles model initialization and provides methods for transcribing audio files.
    """
    
    def __init__(self, model_name="large-v2", device="auto", compute_type="int8"):
        """
        Initialize the ASR transcriber with specified model settings.
        
        Args:
            model_name (str): Name of the Whisper model to use
            device (str): Device to run the model on ('cpu', 'cuda', or 'auto')
            compute_type (str): Computation type for model inference
        """
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
        print(f"ğŸ”§ Initializing Whisper model: {model_name}")
        print(f"ğŸ’» Using device: {device}")
        print(f"ğŸ§® Compute type: {compute_type}")

        self.model_name = model_name
        self.device = device
        self.compute_type = compute_type

        self.model = WhisperModel(
            model_name, 
            device=device, 
            compute_type=compute_type, 
            cpu_threads=16, 
            num_workers=1
        )

    def transcribe(self, audio_path, language="zh"):
        """
        Transcribe an audio file.
        
        Args:
            audio_path (str): Path to the audio file
            language (str): Language code for transcription (default: 'zh')
            
        Returns:
            tuple: (transcribed_text, detected_language)
        """
        segments, info = self.model.transcribe(
            audio_path, 
            beam_size=1,  # Reduced from 5 for 5x speed improvement
            language=language,
            temperature=0.0,  # Fixed temperature
            vad_filter=True,  # Filter silent parts
            vad_parameters=dict(min_silence_duration_ms=300),
            condition_on_previous_text=False
        )
        
        text = " ".join([seg.text for seg in segments]).strip()
        return text, info.language
            
if __name__ == "__main__":
    import time
    import os
    from datetime import datetime
    segments_dir = "outputs/session_20250731_195445/segments"  # æ›¿æ¢ä¸ºå®é™…çš„éŸ³é¢‘æ–‡ä»¶ç›®å½•
    
    if os.path.exists(segments_dir):
        audio_files = sorted([f for f in os.listdir(segments_dir) if f.endswith('.wav')])
        
        if audio_files:

            model_load_start = time.time()

            asr = ASRTranscriber(model_name="large-v2", device="auto")

            model_load_time = time.time() - model_load_start
            print(f"âš¡ æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {model_load_time:.2f}ç§’")
            
            # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
            processing_start_time = time.time()
            results = []
            
            for audio_file in audio_files:
                audio_path = os.path.join(segments_dir, audio_file)
                print(f"\nğŸµ æ­£åœ¨å¤„ç†: {audio_file}")
                
                start_time = time.time()
                text, language = asr.transcribe(audio_path)
                elapsed_time = time.time() - start_time
                
                print(f"ğŸ“ æ–‡æœ¬: {text}")
                print(f"ğŸŒ è¯­è¨€: {language}")
                print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")

                results.append({
                    'file': audio_file,
                    'text': text,
                    'language': language,
                    'duration': elapsed_time
                })
            
            processing_time = time.time() - processing_start_time
            total_time = time.time() - model_load_start
            
            print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(audio_files)}")
            print(f"âš¡ æ¨¡å‹åŠ è½½æ—¶é—´: {model_load_time:.2f}ç§’")
            print(f"ğŸ”„ çº¯å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(f"â±ï¸  æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"ğŸ“ˆ å¹³å‡å¤„ç†æ—¶é—´: {processing_time/len(audio_files):.2f}ç§’/æ–‡ä»¶")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = asr.model_name
            output_file = f"{segments_dir}/{model_name}_transcription_results_{timestamp}.txt"
            
            # å°†ç»“æœå†™å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"è¯­éŸ³è¯†åˆ«ç»“æœæŠ¥å‘Š\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å¤„ç†æ–‡ä»¶æ•°: {len(audio_files)}\n")
                f.write(f"æ¨¡å‹åŠ è½½æ—¶é—´: {model_load_time:.2f}ç§’\n")
                f.write(f"çº¯å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’\n")
                f.write(f"æ€»æ—¶é—´: {total_time:.2f}ç§’\n")
                f.write(f"å¹³å‡å¤„ç†æ—¶é—´: {processing_time/len(audio_files):.2f}ç§’/æ–‡ä»¶\n")
                f.write("="*50 + "\n\n")
                
                for i, result in enumerate(results, 1):
                    f.write(f"[{i:03d}] {result['file']}\n")
                    f.write(f"è¯­è¨€: {result['language']}\n")
                    f.write(f"è€—æ—¶: {result['duration']:.2f}ç§’\n")
                    f.write(f"å†…å®¹: {result['text']}\n")
                    f.write("-" * 30 + "\n\n")
            
            print(f"ğŸ“„ è½¬å½•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ° .wav æ–‡ä»¶")
    else:
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {segments_dir}")



